{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f7690e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#텐서플로우는 구글이 2015년에 공개한 머신 러닝 오픈소스 라이브러리입니다. 머신 러닝과 딥 러닝을 직관적이고 손쉽게 할 수 있도록 설계되었습니다. 뒤의 딥 러닝 실습을 위해서 텐서플로우를 설치해야 합니다.\n",
    "\n",
    "#!pip install keras\n",
    "#케라스\n",
    "#케라스(Keras)는 딥 러닝 프레임워크인 텐서플로우에 대한 추상화 된 API를 제공합니다. 케라스는 백엔드로 텐서플로우를 사용하며, 좀 더 쉽게 딥 러닝을 사용할 수 있게 해줍니다. 쉽게 말해, 텐서플로우 코드를 훨씬 간단하게 작성할 수 있습니다.\n",
    "\n",
    "#PyKoSpacing 사용해보기\n",
    "#!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "\n",
    "#!pip install gensim\n",
    "#젠심(Gensim)은 머신 러닝을 사용하여 토픽 모델링과 자연어 처리 등을 수행할 수 있게 해주는 오픈 소스 라이브러리입니다. 이 책에서도 젠심을 사용하여 Word2Vec 등 다양한 모델들을 학습해볼 것입니다.\n",
    "\n",
    "#!pip install scikit-learn\n",
    "\n",
    "#사이킷런(Scikit-learn)은 파이썬 머신러닝 라이브러리입니다. 사이킷런을 통해 나이브 베이즈 분류, 서포트 벡터 머신 등 다양한 머신 러닝 모듈을 불러올 수 있습니다. 또한, 사이킷런에는 머신러닝을 연습하기 위한 아이리스 데이터, 당뇨병 데이터 등 자체 데이터 또한 제공하고 있습니다. 사이킷런은 위 패키지들과 달리 아나콘다로 자동 설치되지만 아나콘다를 설치하지 않았다면 아래의 커맨드로 Scikit-learn을 별도 설치할 수 있습니다.\n",
    "\n",
    "#!pip install nltk\n",
    "#엔엘티케이(NLTK)는 자연어 처리를 위한 파이썬 패키지입니다. 아나콘다를 설치하였다면 NLTK는 기본적으로 설치가 되어져 있습니다. 아나콘다를 설치하지 않았다면 아래의 커맨드로 NLTK를 별도 설치할 수 있습니다.\n",
    "\n",
    "#!pip install konlpy\n",
    "#코엔엘파이(KoNLPy)는 한국어 자연어 처리를 위한 형태소 분석기 패키지입니다. 프롬프트에서 아래 커맨드로 설치합니다.\n",
    "\n",
    "#!pip install -U pandas-profiling\n",
    "#좋은 요리를 위해서는 조리 방법도 중요하지만, 그만큼 중요한 것은 갖고있는 재료의 상태입니다. 재료가 상하거나 문제가 있다면 좋은 요리가 나올 수 없습니다. 마찬가지로 좋은 머신 러닝 결과를 얻기 위해서는 데이터의 성격을 파악하는 과정이 선행되어야 합니다. 이 과정에서 데이터 내 값의 분포, 변수 간의 관계, Null 값과 같은 결측값(missing values) 존재 유무 등을 파악하게 되는데 이와 같이 데이터를 파악하는 과정을 EDA(Exploratory Data Analysis, 탐색적 데이터 분석)이라고 합니다. 이번에는 방대한 양의 데이터를 가진 데이터프레임을 .profile_report()라는 단 한 줄의 명령으로 탐색하는 패키지인 판다스 프로파일링(pandas-profiling)을 소개합니다.\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('punkt') <-- 해당 대로 설치 진행 \n",
    "\n",
    "#!pip install kss\n",
    "#NLTK는 단순히 마침표를 구분자로 하여 문장을 구분하지 않았기 때문에, Ph.D.를 문장 내의 단어로 인식하여 성공적으로 인식하는 것을 볼 수 있습니다. 한국어에 대한 문장 토큰화 도구 또한 존재합니다. 한국어의 경우에는 박상길님이 개발한 KSS(Korean Sentence Splitter)를 추천합니다. 다음과 같이 KSS를 설치합니다.\n",
    "\n",
    "#!pip install soynlp\n",
    "#soynlp는 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저입니다. 비지도 학습으로 단어 토큰화를 한다는 특징을 갖고 있으며, 데이터에 자주 등장하는 단어들을 단어로 분석합니다. soynlp 단어 토크나이저는 내부적으로 단어 점수 표로 동작합니다. 이 점수는 응집 확률(cohesion probability)과 브랜칭 엔트로피(branching entropy)를 활용합니다.\n",
    "\n",
    "\n",
    "#더 자세한 정보는 아래 사이트 보기\n",
    "#https://wikidocs.net/21703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77310fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#word_tokenize 해보기\n",
    "\n",
    "print('단어 토큰화1 :', word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "\n",
    "\n",
    "#아래 에러\n",
    "\n",
    "#Resource punkt not found. Please use the NLTK Downloader to obtain the resource:\n",
    "    \n",
    "#>>> import nltk\n",
    "#>>> nltk.download('punkt')  이전 탭에서 다운로드 설치 진행 완료 및 문제 해결 끝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "790d7812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰화 : ['띄어쓰기가 될 때, 문장부호의 역할이 중요한 듯 하니 띄어쓰기 먼저 하고 문장부호 제거를 해야겠다.', '문장부호 제거를 어느 순서로 하느냐도 좀 고려해 봐야할 필요가 있을 것 같다.']\n"
     ]
    }
   ],
   "source": [
    "import kss  #한국어 문장 토큰화 (리스트화)\n",
    "\n",
    "text = '띄어쓰기가 될 때, 문장부호의 역할이 중요한 듯 하니 띄어쓰기 먼저 하고 문장부호 제거를 해야겠다. 문장부호 제거를 어느 순서로 하느냐도 좀 고려해 봐야할 필요가 있을 것 같다.'\n",
    "print('한국어 문장 토큰화 :',kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6be9b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "띄어쓰기가, 문장부호의 역할이 중요한 띄어쓰기 문장부호 제거를 해야겠다. 문장부호 제거를 순서로 하느냐도 고려해 봐야할 필요가.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"띄어쓰기가 될 때, 문장부호의 역할이 중요한 듯 하니 띄어쓰기 먼저 하고 문장부호 제거를 해야겠다. 문장부호 제거를 어느 순서로 하느냐도 좀 고려해 봐야할 필요가 있을 것 같다.\"\n",
    "\n",
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32229ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
      "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from konlpy.tag import Okt\n",
    "\n",
    "#한국어 불용어 제거하기\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
    "\n",
    "stop_words = set(stop_words.split(' '))\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cca34771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "띄어쓰기가 될 때, 문장부호의 역할이 중요한 듯 하니 띄어쓰기 먼저 하고 문장부호 제거를 해야겠다. 문장부호 제거를 어느 순서로 하느냐도 좀 고려해 봐야할 필요가 있을 것 같다.\n",
      "띄어쓰기가 될 때, 문장부호의 역할이 중요한 듯 하니 띄어 쓰기 먼저 하고 문장부호 제거를 해야겠다.문 장부호 제거를 어느 순서로 하느냐도 좀 고려해봐야 할 필요가 있을 것 같다.\n"
     ]
    }
   ],
   "source": [
    "from pykospacing import Spacing\n",
    "\n",
    "#문장 띄어쓰기\n",
    "\n",
    "sent = '띄어쓰기가 될 때, 문장부호의 역할이 중요한 듯 하니 띄어쓰기 먼저 하고 문장부호 제거를 해야겠다. 문장부호 제거를 어느 순서로 하느냐도 좀 고려해 봐야할 필요가 있을 것 같다.'\n",
    "\n",
    "new_sent = sent.replace(\" \", '') # 띄어쓰기가 없는 문장 임의로 만들기\n",
    "#print(new_sent)\n",
    "\n",
    "spacing = Spacing()\n",
    "kospacing_sent = spacing(new_sent) \n",
    "\n",
    "print(sent)\n",
    "print(kospacing_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d97f76f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "띄어쓰기가 될 때, 문장부호의 역할이 중요한 듯하니 띄어쓰기 먼저 하고 문장부호 제거를 해야겠다. 문장부호 제거를 어느 순서로 하느냐도 좀 고려해 봐야 할 필요가 있을 것 같다.\n"
     ]
    }
   ],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "#문장 맞춤법\n",
    "\n",
    "sent = \"띄어쓰기가 될 때, 문장부호의 역할이 중요한 듯 하니 띄어쓰기 먼저 하고 문장부호 제거를 해야겠다. 문장부호 제거를 어느 순서로 하느냐도 좀 고려해 봐야할 필요가 있을 것 같다.\"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e415f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "띄어쓰기가 될 때, 문장부호의 역할이 중요한듯하네 띄어쓰기 먼저 하고 문장부호 제거를 해야겠다. 문장부호 제거를 어느 순서로 하느냐도 좀 고려해봐야 할 필요가 있을 것 같다.\n",
      "띄어쓰기가 될 때, 문장부호의 역할이 중요한 듯 하니 띄어 쓰기 먼저 하고 문장부호 제거를 해야겠다.문 장부호 제거를 어느 순서로 하느냐도 좀 고려해봐야 할 필요가 있을 것 같다.\n"
     ]
    }
   ],
   "source": [
    "spelled_sent = spell_checker.check(new_sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)\n",
    "print(kospacing_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "defc71d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "#신조어 문제\n",
    "\n",
    "tokenizer = Okt()\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
